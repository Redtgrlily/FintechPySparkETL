# FintechPySparkETL ğŸš€  
> End-to-end PySpark pipeline for fintech transaction analysis, fraud detection using machine learning, and interactive dashboarding via Streamlit.

## ğŸ“Œ Overview

**FintechPySparkETL** is a modular, scalable, and production-oriented data pipeline built using Apache Spark. The project performs the following:

- ğŸ§¼ **ETL**: Ingests raw transaction data and cleanses, transforms, and enriches it.
- ğŸ¤– **Machine Learning**: Flags potentially fraudulent transactions using a trained classification model.
- ğŸ“Š **Dashboard**: A Streamlit-based UI for visualizing and reviewing suspicious activity.

This pipeline is designed with performance, transparency, and extensibility in mind, ideal for real-world fintech data operations.

---

## ğŸ—‚ï¸ Project Structure
FintechPySparkETL/
â”‚
â”œâ”€â”€ data/ # Input and output datasets
â”‚ â””â”€â”€ sample_transactions.csv
â”‚
â”œâ”€â”€ etl/ # PySpark ETL job scripts
â”‚ â””â”€â”€ fraud_etl_pipeline.py
â”‚
â”œâ”€â”€ ml/ # ML pipeline and model training
â”‚ â””â”€â”€ generate_ml_data.py
â”‚ â””â”€â”€ fraud_model.py
â”‚
â”œâ”€â”€ dashboard/ # Streamlit dashboard
â”‚ â””â”€â”€ app.py
â”‚
â”œâ”€â”€ _archive/ # Archived local dev files (excluded from repo)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ run_etl.sh # Shell script to run ETL job


---

## âš™ï¸ Technologies Used

- **Apache Spark (PySpark)** â€” distributed data processing
- **Scikit-learn** â€” ML model training (Random Forest)
- **Streamlit** â€” interactive dashboard
- **Pandas / NumPy / Faker** â€” data manipulation and generation
- **Git** â€” version control

---

## ğŸ” Features

- âœ… Batch ETL processing using Spark
- âœ… Feature engineering for fraud detection
- âœ… Trained ML model with fraud flagging
- âœ… Real-time data exploration via Streamlit
- âœ… Clean structure and scalable architecture

---

## ğŸš€ Getting Started

### 1. Install Prerequisites

- [Install Spark 3.5.1](https://spark.apache.org/downloads.html)
- Python 3.9+
- Java 11+
- Git

### 2. Clone the Repository

```bash
git clone https://github.com/Redtgrlily/FintechPySparkETL.git
cd FintechPySparkETL

### 3. Set Up Environment
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
pip install -r requirements.txt

### 4. Generate Sample Data
python ml/generate_ml_data.py

### 5. Run the ETL Job
spark-submit etl/fraud_etl_pipeline.py

### 6. Launch the Dashboard
cd dashboard
streamlit run app.py

### Sample Output
data/processed_transactions.csv: includes features + fraud prediction column

Streamlit dashboard allows filtering, exploring, and visualizing fraud patterns.

### .gitignore Highlights
This project excludes:

Spark binaries

Large data files

Environment folders

Local IDE/project settings

See .gitignore for details.

### Contributing
Contributions are welcome! Fork the repo, make your changes, and open a pull request.

###  License
MIT License. See LICENSE file for details.

### Author
Kaitlyn McCormick
github.com/Redtgrlily